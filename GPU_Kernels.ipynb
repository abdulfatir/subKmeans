{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "  #include <stdio.h>\n",
    "  #include <math.h>\n",
    "\n",
    "  __global__ void matmul(float *a, float *b, float *c, int *a_shape, int *b_shape)\n",
    "  {\n",
    "      if((blockDim.y * blockIdx.y + threadIdx.y) < a_shape[0] && (blockDim.x * blockIdx.x + threadIdx.x) < b_shape[1])\n",
    "      {\n",
    "        int aMin = (blockDim.y * blockIdx.y + threadIdx.y) * a_shape[1]; \n",
    "        int aMax = (blockDim.y * blockIdx.y + threadIdx.y + 1) *  a_shape[1]; \n",
    "        int aStep = 1;\n",
    "        int bMin = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int bMax = blockDim.x * blockIdx.x + threadIdx.x + b_shape[0]*b_shape[1];\n",
    "        int bStep = b_shape[1];\n",
    "        float temp = 0;\n",
    "        for(int ai=aMin, bi = bMin; ai < aMax && bi < bMax; ai += aStep, bi += bStep)\n",
    "        {\n",
    "                temp += a[ai] * b[bi];\n",
    "        }\n",
    "        int a_index = (blockDim.y * blockIdx.y + threadIdx.y) * b_shape[1];\n",
    "        c[a_index+bMin] = temp;\n",
    "    } \n",
    "  }\n",
    "  __global__ void transpose(float *a, float *a_T, int *a_shape) \n",
    "  {\n",
    "      int elem_idx = (blockDim.y * blockIdx.y + threadIdx.y) * a_shape[1] +  blockDim.x * blockIdx.x + threadIdx.x;\n",
    "      if (elem_idx < a_shape[0]*a_shape[1]) \n",
    "          {\n",
    "              int a_t_1 = a_shape[0];\n",
    "              int elem_tr_idx =  (blockDim.x * blockIdx.x + threadIdx.x) * a_t_1 +  blockDim.y * blockIdx.y + threadIdx.y;\n",
    "              a_T[elem_tr_idx] = a[elem_idx];\n",
    "          }\n",
    "  \n",
    "  }\n",
    "  \n",
    "  __global__ void row_mean(float *a, float *mean, int *a_shape)\n",
    "  {\n",
    "  //Returns a column\n",
    "      int row_num = (blockDim.x * blockIdx.x + threadIdx.x);\n",
    "      if (row_num < a_shape[0])\n",
    "      {\n",
    "          int start_idx = row_num*a_shape[1];\n",
    "          int end_idx = start_idx + a_shape[1];\n",
    "          float sum = 0;\n",
    "          for (int i = start_idx; i< end_idx; i++) \n",
    "          {\n",
    "              sum += a[i];\n",
    "          }\n",
    "          mean[row_num] = sum/a_shape[1];\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  __global__ void column_mean(float *a, float *mean, int *a_shape)\n",
    "  {\n",
    "  //Returns a row\n",
    "      int col_num = (blockDim.x * blockIdx.x + threadIdx.x);\n",
    "      if (col_num < a_shape[1])\n",
    "      {\n",
    "          int start_idx = col_num;\n",
    "          int end_idx = start_idx + a_shape[1]*a_shape[0];\n",
    "          float sum = 0;\n",
    "          for (int i = start_idx; i< end_idx; i+= a_shape[1]) \n",
    "          {\n",
    "              sum += a[i];\n",
    "          }\n",
    "          mean[col_num] = sum/a_shape[0];\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  __global__ void min_row(float *a, int *a_shape, float *min_row, int *arg_min)\n",
    "  {\n",
    "    //Returns a column for min_row and argmin \n",
    "      int row_num = (blockDim.x * blockIdx.x + threadIdx.x);\n",
    "      if (row_num < a_shape[0])\n",
    "      {\n",
    "          int start_idx = row_num*a_shape[1];\n",
    "          int end_idx = start_idx + a_shape[1];\n",
    "          min_row[row_num] = a[start_idx];\n",
    "          arg_min[row_num] = 0;\n",
    "          for (int col = start_idx+1, index=1; col< end_idx, index < a_shape[1]; col++, index ++) \n",
    "          {\n",
    "              if (a[col] < min_row[row_num])\n",
    "              {\n",
    "                  min_row[row_num] = a[col];\n",
    "                  arg_min[row_num] = index;\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "  \n",
    "  }\n",
    "  \n",
    "  __global__ void sum_axis3(float *a, int *a_shape, float *result)\n",
    "  {\n",
    "      //a[i][j][k] = k+a_shape[2]*j + a_shape[2]*a_shape[1]*i\n",
    "      \n",
    "      int col_num = (blockDim.x * blockIdx.x + threadIdx.x);\n",
    "      int row_num = (blockDim.y * blockIdx.y + threadIdx.y);\n",
    "      if (row_num < a_shape[0] && col_num < a_shape[1])\n",
    "      {\n",
    "          int start_idx =(row_num*a_shape[1] + col_num)*a_shape[2];\n",
    "          int end_idx = start_idx + a_shape[2];\n",
    "          int step = 1;\n",
    "          float temp = 0;\n",
    "          for (int idx = start_idx; idx < end_idx; idx+= step) \n",
    "          {\n",
    "              temp += a[idx];\n",
    "          }\n",
    "          result[row_num*a_shape[1] + col_num] = temp;\n",
    "      }\n",
    "  \n",
    "  }\n",
    "  \n",
    "    __global__ void sum_axis2(float *a, int *a_shape, float *result)\n",
    "  {\n",
    "      //a[i][j][k] = k+a_shape[2]*j + a_shape[2]*a_shape[1]*i\n",
    "      \n",
    "      int col_num = (blockDim.x * blockIdx.x + threadIdx.x);\n",
    "      int row_num = (blockDim.y * blockIdx.y + threadIdx.y);\n",
    "      if (row_num < a_shape[0] && col_num < a_shape[2])\n",
    "      {\n",
    "          int start_idx =row_num*a_shape[1]*a_shape[2] + col_num;\n",
    "          int end_idx = start_idx + a_shape[2]*a_shape[1];\n",
    "          int step = a_shape[2];\n",
    "          float temp = 0;\n",
    "          for (int idx = start_idx; idx < end_idx; idx+= step) \n",
    "          {\n",
    "              temp += a[idx];\n",
    "          }\n",
    "          result[row_num*a_shape[2] + col_num] = temp;\n",
    "      }\n",
    "  \n",
    "  }\n",
    "  \n",
    "    __global__ void sum_axis1(float *a, int *a_shape, float *result)\n",
    "  {\n",
    "      //a[i][j][k] = k+a_shape[2]*j + a_shape[2]*a_shape[1]*i\n",
    "      \n",
    "      int col_num = (blockDim.x * blockIdx.x + threadIdx.x);\n",
    "      int row_num = (blockDim.y * blockIdx.y + threadIdx.y);\n",
    "      if (row_num < a_shape[1] && col_num < a_shape[2])\n",
    "      {\n",
    "          int start_idx =(row_num)*a_shape[2] + col_num;\n",
    "          int end_idx = start_idx + a_shape[2]*a_shape[1]*a_shape[0];\n",
    "          int step = a_shape[2]*a_shape[1];\n",
    "          float temp = 0;\n",
    "          for (int idx = start_idx; idx < end_idx; idx+= step) \n",
    "          {\n",
    "              temp += a[idx];\n",
    "          }\n",
    "          result[row_num*a_shape[2] + col_num] = temp;\n",
    "      }\n",
    "  \n",
    "  }\n",
    "  \n",
    "      __global__ void argmin_mu_diff(float *data, float *mu, int *data_shape, int *mu_shape, int *arg_min)\n",
    "  {\n",
    "      \n",
    "      int data_id = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "      if (data_id < data_shape[0] )\n",
    "      {\n",
    "          int startIdx = (blockDim.x * blockIdx.x + threadIdx.x)*data_shape[1];\n",
    "          float min_diff = INT_MAX;\n",
    "          float arg_min_diff = -1;\n",
    "          for (int i=0; i<mu_shape[0]; i++) \n",
    "          {\n",
    "              float diff = 0;\n",
    "              for (int dim = 0; dim < mu_shape[1]; dim ++)\n",
    "              {\n",
    "                  diff += (data[startIdx+dim] - mu[i*mu_shape[1] + dim])*(data[startIdx+dim] - mu[i*mu_shape[1] + dim]);\n",
    "              }\n",
    "              if (diff < min_diff)\n",
    "              {\n",
    "                  min_diff = diff;\n",
    "                  arg_min_diff = i;\n",
    "              }\n",
    "          }\n",
    "          arg_min[data_id] = arg_min_diff;\n",
    "      }\n",
    "  \n",
    "  }\n",
    "  \n",
    "  \n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(10549, 8982).astype(np.float32)\n",
    "b = np.random.randn(8982, 10549).astype(np.float32)\n",
    "c = np.zeros([a.shape[0], b.shape[1]]).astype(np.float32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)\n",
    "SHAPE_B = np.array(b.shape).astype(np.uint32)\n",
    "print(SHAPE_A, SHAPE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "cuda.memcpy_htod(c_gpu, c)\n",
    "\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)\n",
    "SHAPE_B_gpu = cuda.mem_alloc(SHAPE_B.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_B_gpu, SHAPE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"matmul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_DIMX = 32\n",
    "BLOCK_DIMY = 32\n",
    "GRID_DIMX = int(np.ceil(b.shape[1]/float(BLOCK_DIMX)))\n",
    "GRID_DIMY = int(np.ceil(a.shape[0]/float(BLOCK_DIMY)))\n",
    "\n",
    "print (GRID_DIMX, GRID_DIMY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "func(a_gpu, b_gpu, c_gpu, SHAPE_A_gpu, SHAPE_B_gpu, block=(BLOCK_DIMX, BLOCK_DIMY, 1), grid=(GRID_DIMX, GRID_DIMY, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.empty_like(c)\n",
    "cuda.memcpy_dtoh(results, c_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.matmul(a, b), results, atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRANSPOSE##\n",
    "a = np.random.randn(10549, 8982).astype(np.float32)\n",
    "a_T = np.zeros(list(reversed(a.shape))).astype(np.float32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "a_T_gpu = cuda.mem_alloc(a_T.nbytes)\n",
    "cuda.memcpy_htod(a_T_gpu, a_T)\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"transpose\")\n",
    "BLOCK_DIMX = 32\n",
    "BLOCK_DIMY = 32\n",
    "GRID_DIMX = int(np.ceil(a.shape[1]/float(BLOCK_DIMX)))\n",
    "GRID_DIMY = int(np.ceil(a.shape[0]/float(BLOCK_DIMY)))\n",
    "print (GRID_DIMX, GRID_DIMY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "func(a_gpu, a_T_gpu, SHAPE_A_gpu, block=(BLOCK_DIMX, BLOCK_DIMY, 1), grid=(GRID_DIMX, GRID_DIMY, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.empty_like(a_T)\n",
    "cuda.memcpy_dtoh(results, a_T_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.transpose(a), results, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.transpose(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROW MEAN ##\n",
    "a = np.random.randn(10549, 8982).astype(np.float32)\n",
    "mean_a = np.zeros([a.shape[0]]).astype(np.float32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "mean_a_gpu = cuda.mem_alloc(mean_a.nbytes)\n",
    "cuda.memcpy_htod(mean_a_gpu, mean_a)\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"row_mean\")\n",
    "BLOCK_DIMX = 1024\n",
    "GRID_DIMX = int(np.ceil(a.shape[0]/float(BLOCK_DIMX)))\n",
    "print (GRID_DIMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 µs, sys: 90 µs, total: 622 µs\n",
      "Wall time: 626 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, mean_a_gpu, SHAPE_A_gpu, block=(BLOCK_DIMX, 1, 1), grid=(GRID_DIMX, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01379276 -0.01639137 -0.00990051 ...,  0.00314257  0.0034696\n",
      "  0.00319359]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(mean_a)\n",
    "cuda.memcpy_dtoh(results, mean_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.mean(a, axis=1), results, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 ms, sys: 98 µs, total: 40.8 ms\n",
      "Wall time: 40.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01379275, -0.01639137, -0.00990053, ...,  0.00314256,\n",
       "        0.0034696 ,  0.00319358], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.mean(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLUMN MEAN ##\n",
    "a = np.random.randn(10549, 8982).astype(np.float32)\n",
    "mean_a = np.zeros([a.shape[1]]).astype(np.float32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "mean_a_gpu = cuda.mem_alloc(mean_a.nbytes)\n",
    "cuda.memcpy_htod(mean_a_gpu, mean_a)\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"column_mean\")\n",
    "BLOCK_DIMX = 1024\n",
    "GRID_DIMX = int(np.ceil(a.shape[1]/float(BLOCK_DIMX)))\n",
    "print (GRID_DIMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 µs, sys: 8 µs, total: 63 µs\n",
      "Wall time: 66 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, mean_a_gpu, SHAPE_A_gpu, block=(BLOCK_DIMX, 1, 1), grid=(GRID_DIMX, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00875998  0.01302794 -0.00799079 ..., -0.01028944 -0.00198964\n",
      "  0.00089069]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(mean_a)\n",
    "cuda.memcpy_dtoh(results, mean_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.mean(a, axis=0), results, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.2 ms, sys: 160 µs, total: 37.4 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00875998,  0.01302794, -0.00799079, ..., -0.01028944,\n",
       "       -0.00198964,  0.00089069], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.mean(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARGMIN, MIN ##\n",
    "a = np.random.randn(10549, 8982).astype(np.float32)\n",
    "min_a = np.zeros([a.shape[0]]).astype(np.float32)\n",
    "argmin_a = np.zeros([a.shape[0]]).astype(np.uint32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "min_a_gpu = cuda.mem_alloc(min_a.nbytes)\n",
    "cuda.memcpy_htod(min_a_gpu, min_a)\n",
    "argmin_a_gpu = cuda.mem_alloc(argmin_a.nbytes)\n",
    "cuda.memcpy_htod(argmin_a_gpu, argmin_a)\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"min_row\")\n",
    "BLOCK_DIMX = 1024\n",
    "GRID_DIMX = int(np.ceil(a.shape[0]/float(BLOCK_DIMX)))\n",
    "print (GRID_DIMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71 µs, sys: 9 µs, total: 80 µs\n",
      "Wall time: 83 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, SHAPE_A_gpu, min_a_gpu, argmin_a_gpu, block=(BLOCK_DIMX, 1, 1), grid=(GRID_DIMX, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.85049248 -3.55566049 -4.19751215 ..., -3.73588705 -4.49926805\n",
      " -3.76275706]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(min_a)\n",
    "cuda.memcpy_dtoh(results, min_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.min(a, axis=1), results, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 ms, sys: 0 ns, total: 27.5 ms\n",
      "Wall time: 26.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.85049248, -3.55566049, -4.19751215, ..., -3.73588705,\n",
       "       -4.49926805, -3.76275706], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.min(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6349  448 1200 ..., 6421 6132 8354]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(argmin_a)\n",
    "cuda.memcpy_dtoh(results, argmin_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.argmin(a, axis=1), results, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUM AXIS##\n",
    "a = np.random.randn(1054, 89,45).astype(np.float32)\n",
    "sum3_a = np.zeros([a.shape[0], a.shape[1]]).astype(np.float32)\n",
    "sum2_a = np.zeros([a.shape[0], a.shape[2]]).astype(np.float32)\n",
    "sum1_a = np.zeros([a.shape[1], a.shape[2]]).astype(np.float32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "\n",
    "sum3_a_gpu = cuda.mem_alloc(sum3_a.nbytes)\n",
    "cuda.memcpy_htod(sum3_a_gpu, sum3_a)\n",
    "sum2_a_gpu = cuda.mem_alloc(sum2_a.nbytes)\n",
    "cuda.memcpy_htod(sum2_a_gpu, sum2_a)\n",
    "sum1_a_gpu = cuda.mem_alloc(sum1_a.nbytes)\n",
    "cuda.memcpy_htod(sum1_a_gpu, sum1_a)\n",
    "\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 33)\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"sum_axis3\")\n",
    "BLOCK_DIMX = 32\n",
    "BLOCK_DIMY = 32\n",
    "GRID_DIMX = int(np.ceil(a.shape[1]/float(BLOCK_DIMX)))\n",
    "GRID_DIMY = int(np.ceil(a.shape[0]/float(BLOCK_DIMY)))\n",
    "print (GRID_DIMX, GRID_DIMY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83 µs, sys: 10 µs, total: 93 µs\n",
      "Wall time: 91.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, SHAPE_A_gpu, sum3_a_gpu, block=(BLOCK_DIMX, BLOCK_DIMY, 1), grid=(GRID_DIMX, GRID_DIMY, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.76620173   3.46547246  -6.58266258 ...,  12.91075134   9.27414417\n",
      "   -0.20195746]\n",
      " [ -0.71003604   3.96370769   1.51221001 ...,   1.15611005  -3.51532745\n",
      "    4.20375776]\n",
      " [  5.52328873  -3.01439714   9.02602386 ...,  -6.46910334   5.86660385\n",
      "   -1.304389  ]\n",
      " ..., \n",
      " [  1.89996386   5.34373951  -2.49017596 ...,   5.4591713   -3.82690263\n",
      "    0.73662424]\n",
      " [ 13.79426289   4.93118811  -2.96993637 ...,  -0.7317217  -11.43707466\n",
      "    4.18983364]\n",
      " [ -0.17461634  -8.47156334  -4.94273281 ...,   4.93384695  11.56926537\n",
      "    5.59148455]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(sum3_a)\n",
    "cuda.memcpy_dtoh(results, sum3_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.sum(a, axis=2), results, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.57 ms, sys: 70 µs, total: 4.64 ms\n",
      "Wall time: 4.11 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.76620221,   3.46547151,  -6.58266258, ...,  12.91075134,\n",
       "          9.27414417,  -0.20195735],\n",
       "       [ -0.71003652,   3.96370864,   1.51220906, ...,   1.15611076,\n",
       "         -3.51532745,   4.20375776],\n",
       "       [  5.52328873,  -3.01439714,   9.026021  , ...,  -6.46910286,\n",
       "          5.86660433,  -1.30438852],\n",
       "       ..., \n",
       "       [  1.89996374,   5.34373856,  -2.49017549, ...,   5.45917034,\n",
       "         -3.82690263,   0.73662472],\n",
       "       [ 13.79426193,   4.93118811,  -2.96993589, ...,  -0.73172122,\n",
       "        -11.43707466,   4.18983078],\n",
       "       [ -0.17461705,  -8.47156334,  -4.94273329, ...,   4.93384743,\n",
       "         11.56926346,   5.59148455]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.sum(a, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 33)\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"sum_axis2\")\n",
    "BLOCK_DIMX = 32\n",
    "BLOCK_DIMY = 32\n",
    "GRID_DIMX = int(np.ceil(a.shape[2]/float(BLOCK_DIMX)))\n",
    "GRID_DIMY = int(np.ceil(a.shape[0]/float(BLOCK_DIMY)))\n",
    "print (GRID_DIMX, GRID_DIMY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62 µs, sys: 8 µs, total: 70 µs\n",
      "Wall time: 66 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, SHAPE_A_gpu, sum2_a_gpu, block=(BLOCK_DIMX, BLOCK_DIMY, 1), grid=(GRID_DIMX, GRID_DIMY, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.35563946  -2.84267688  -4.73142624 ...,   9.26231384   8.08987999\n",
      "   -5.98818731]\n",
      " [  5.91100359   6.87636423   4.91737843 ..., -10.0587616    3.02491474\n",
      "   -8.86170101]\n",
      " [  4.76445818  -4.81286144  -2.51421356 ...,  -9.17828274  10.49326992\n",
      "    5.56969309]\n",
      " ..., \n",
      " [ 13.83674335 -12.90276909   9.26255608 ..., -11.63795757  17.81700706\n",
      "   -0.82999754]\n",
      " [  9.45636177   9.15571785  17.81391525 ...,  -1.41644168   3.24659348\n",
      "   -5.94546223]\n",
      " [  4.4145093   21.88774109   5.29872561 ...,   1.59486461  -2.7983191\n",
      "   12.5598917 ]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(sum2_a)\n",
    "cuda.memcpy_dtoh(results, sum2_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.sum(a, axis=1), results, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.22 ms, sys: 27 µs, total: 4.25 ms\n",
      "Wall time: 3.88 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-10.35563946,  -2.84267688,  -4.73142624, ...,   9.26231384,\n",
       "          8.08987999,  -5.98818731],\n",
       "       [  5.91100359,   6.87636423,   4.91737843, ..., -10.0587616 ,\n",
       "          3.02491474,  -8.86170101],\n",
       "       [  4.76445818,  -4.81286144,  -2.51421356, ...,  -9.17828274,\n",
       "         10.49326992,   5.56969309],\n",
       "       ..., \n",
       "       [ 13.83674335, -12.90276909,   9.26255608, ..., -11.63795757,\n",
       "         17.81700706,  -0.82999754],\n",
       "       [  9.45636177,   9.15571785,  17.81391525, ...,  -1.41644168,\n",
       "          3.24659348,  -5.94546223],\n",
       "       [  4.4145093 ,  21.88774109,   5.29872561, ...,   1.59486461,\n",
       "         -2.7983191 ,  12.5598917 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.sum(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"sum_axis1\")\n",
    "BLOCK_DIMX = 32\n",
    "BLOCK_DIMY = 32\n",
    "GRID_DIMX = int(np.ceil(a.shape[2]/float(BLOCK_DIMX)))\n",
    "GRID_DIMY = int(np.ceil(a.shape[1]/float(BLOCK_DIMY)))\n",
    "print (GRID_DIMX, GRID_DIMY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 µs, sys: 8 µs, total: 68 µs\n",
      "Wall time: 68.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, SHAPE_A_gpu, sum1_a_gpu, block=(BLOCK_DIMX, BLOCK_DIMY, 1), grid=(GRID_DIMX, GRID_DIMY, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-73.16607666  27.83765411  20.21465492 ..., -23.16413307  71.95492554\n",
      "  -48.57486343]\n",
      " [ 63.09356689 -55.15248108  72.82141876 ..., -30.33496284 -79.31492615\n",
      "  -47.61728668]\n",
      " [ 49.15576172  28.31169701 -13.40281773 ...,  40.66150665 -65.16136932\n",
      "    2.33651495]\n",
      " ..., \n",
      " [  3.4313221   12.96168423  13.49198818 ..., -27.3525219   39.18828583\n",
      "   12.34987354]\n",
      " [ 56.69228745 -24.02456284  -3.21201944 ...,   8.04820061  40.63549042\n",
      "   35.67337036]\n",
      " [  7.17916489 -11.18802643 -32.46915054 ...,  24.59671783  26.51755524\n",
      "   10.77449417]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(sum1_a)\n",
    "cuda.memcpy_dtoh(results, sum1_a_gpu)\n",
    "print(results)\n",
    "print(np.allclose(np.sum(a, axis=0), results, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.49 ms, sys: 314 µs, total: 2.81 ms\n",
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-73.16607666,  27.83765411,  20.21465492, ..., -23.16413307,\n",
       "         71.95492554, -48.57486343],\n",
       "       [ 63.09356689, -55.15248108,  72.82141876, ..., -30.33496284,\n",
       "        -79.31492615, -47.61728668],\n",
       "       [ 49.15576172,  28.31169701, -13.40281773, ...,  40.66150665,\n",
       "        -65.16136932,   2.33651495],\n",
       "       ..., \n",
       "       [  3.4313221 ,  12.96168423,  13.49198818, ..., -27.3525219 ,\n",
       "         39.18828583,  12.34987354],\n",
       "       [ 56.69228745, -24.02456284,  -3.21201944, ...,   8.04820061,\n",
       "         40.63549042,  35.67337036],\n",
       "       [  7.17916489, -11.18802643, -32.46915054, ...,  24.59671783,\n",
       "         26.51755524,  10.77449417]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MU_DIFF#\n",
    "a = np.random.randn(1054, 89).astype(np.float32)\n",
    "mu = np.random.randn(25, 89).astype(np.float32)\n",
    "SHAPE_A = np.array(a.shape).astype(np.uint32)\n",
    "SHAPE_MU = np.array(mu.shape).astype(np.uint32)\n",
    "argmin = np.zeros(a.shape[0]).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "\n",
    "mu_gpu = cuda.mem_alloc(mu.nbytes)\n",
    "cuda.memcpy_htod(mu_gpu, mu)\n",
    "\n",
    "SHAPE_A_gpu = cuda.mem_alloc(SHAPE_A.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_A_gpu, SHAPE_A)\n",
    "\n",
    "SHAPE_MU_gpu = cuda.mem_alloc(SHAPE_MU.nbytes)\n",
    "cuda.memcpy_htod(SHAPE_MU_gpu, SHAPE_MU)\n",
    "\n",
    "argmin_gpu = cuda.mem_alloc(argmin.nbytes)\n",
    "cuda.memcpy_htod(argmin_gpu, argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"argmin_mu_diff\")\n",
    "BLOCK_DIMX = 1024\n",
    "GRID_DIMX = int(np.ceil(a.shape[0]/float(BLOCK_DIMX)))\n",
    "print (GRID_DIMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 µs, sys: 8 µs, total: 74 µs\n",
      "Wall time: 70.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func(a_gpu, mu_gpu, SHAPE_A_gpu,SHAPE_MU_gpu, argmin_gpu, block=(BLOCK_DIMX, 1, 1), grid=(GRID_DIMX, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 17 16 ..., 18 18  7]\n"
     ]
    }
   ],
   "source": [
    "results = np.empty_like(argmin)\n",
    "cuda.memcpy_dtoh(results, argmin_gpu)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 0 ns, total: 10.1 ms\n",
      "Wall time: 9.52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans = np.argmin(np.sum(np.square(a[:, None, :] - mu[None, :, :]), axis=-1), axis=-1) # n,k,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ans, results, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
